{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random as rd\n",
    "import itertools as itertools\n",
    "\n",
    "# SQL\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy_utils import database_exists, create_database\n",
    "import _mysql\n",
    "\n",
    "# Plotting packages\n",
    "import matplotlib.pyplot as plt\n",
    "from bokeh.plotting import figure, output_notebook, show\n",
    "from bokeh.models import Range1d\n",
    "from bokeh.charts import Bar, output_file, show\n",
    "from bokeh.sampledata.autompg import autompg as df\n",
    "%matplotlib inline\n",
    "output_notebook()\n",
    "\n",
    "# Curve (plot) smoothing \n",
    "from scipy.signal import savgol_filter\n",
    "\n",
    "# kslearn packages\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_curve, auc, precision_recall_curve, precision_score,recall_score\n",
    "from sklearn.metrics import average_precision_score,confusion_matrix, f1_score\n",
    "\n",
    "# regular expression\n",
    "import re\n",
    "\n",
    "mysqlFilePath = 'mysql://root:@localhost/clientsuccess?charset=utf8&use_unicode=0'\n",
    "\n",
    "'''\n",
    "Thie function replaces two special characters that correspond to True and False \n",
    "in the MySQL dump file that I received from the startup company that I consulted for.\n",
    "'''\n",
    "def replace_special(char):\n",
    "    try:\n",
    "        if char.encode('string-escape') == r'\\x01':\n",
    "            return True\n",
    "        elif char.encode('string-escape') == r'\\x00':\n",
    "            return False\n",
    "        else:\n",
    "            return char\n",
    "    except:\n",
    "        return char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = create_engine(mysqlFilePath, pool_recycle=3600)\n",
    "connection = engine.connect()\n",
    "\n",
    "tb_name = \"kim_all_tenants\"\n",
    "tb_result =  pd.read_sql_query(\"\"\"\n",
    "    SELECT \n",
    "        *\n",
    "    FROM \n",
    "        %s\n",
    "        \"\"\"%tb_name, connection).applymap(lambda x: replace_special(x))\n",
    "\n",
    "tb_result = tb_result[tb_result['sentiment'].apply(lambda x: False if np.isnan(x) else True)]\\\n",
    "        .copy(deep=True)\n",
    "tb_res = tb_result[tb_result['amount_per_day'].apply(lambda x: False if np.isnan(x) | np.isinf(x) else True)]\\\n",
    "        .copy(deep=True)\n",
    "tb_result = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def RF(features, Xtrain, ytrain, test_size = 0.3, n_estimators = 100, max_depth = 3):\n",
    "\n",
    "    clf = RandomForestClassifier(n_estimators=n_estimators, max_depth = max_depth)\n",
    "    clf = clf.fit(X_train, y_train)\n",
    "    coeff = clf.get_params()\n",
    "\n",
    "    print coeff\n",
    "\n",
    "    print(\"score: %.4f\" % clf.score(X_test, y_test))\n",
    "    \n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def data_preprocessing(df):\n",
    "\n",
    "    lst_tenants = df['tenant_id'].unique()\n",
    "    #print lst_tenants\n",
    "\n",
    "    for x in lst_tenants:\n",
    "        name = \"tenant\"+str(x)\n",
    "        mask = (df['tenant_id'] == x)\n",
    "        mask = [1 if x == True else 0 for x in mask]\n",
    "        df[name] = pd.Series(mask, index = df.index)\n",
    "\n",
    "    tenant_names = [\"tenant\"+str(x) for x in lst_tenants]\n",
    "    feature_lst = ['client_note_total_count', 'sub_duration', 'last_sub_duration', 'amount_per_day', 'sentiment']\\\n",
    "        + tenant_names\n",
    "    #print feature_lst\n",
    "    X_original = df[feature_lst].as_matrix()\n",
    "    y = np.array(df['churned'])\n",
    "    return X_original, y , feature_lst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def confusion_table_plot(y_test, y_pred):\n",
    "    cnf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    np.set_printoptions(precision=2)\n",
    "\n",
    "    # Plot non-normalized confusion matrix\n",
    "    plt.figure()\n",
    "\n",
    "    class_names = ['Non-Churn','Churn']\n",
    "    plot_confusion_matrix(cnf_matrix, classes=class_names,\n",
    "                          title='Confusion matrix, without normalization')\n",
    "\n",
    "    # Plot normalized confusion matrix\n",
    "    plt.figure()\n",
    "    plot_confusion_matrix(cnf_matrix, classes=class_names, normalize=True,\n",
    "                          title='Normalized confusion matrix')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_original, y, feature_lst = data_preprocessing(tb_res)\n",
    "scaler = StandardScaler()\n",
    "X_raw = scaler.fit_transform(X_original)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_raw, y, test_size=.3, random_state = 100)\n",
    "\n",
    "clf = RF(feature_lst, X_train, y_train, n_estimators = 1000, max_depth = 10)\n",
    "y_pred = clf.predict(X_test)\n",
    "confusion_table_plot(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_score(y_test, y_pred) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recall is one of the important metric to consider, because the algorithm would not want to predict unhappy customers as happy ones. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_importance(feature_importance_lst, feature_lst):\n",
    "    p=figure(plot_width=800, plot_height=200)  \n",
    "    df = pd.DataFrame()\n",
    "    df['importance'] = pd.Series(feature_importance_lst , index = feature_lst)\n",
    "    p = Bar(df, values='importance',title=\"Feature Importance\", legend=False,\\\n",
    "            plot_width=800, plot_height=300)\n",
    "    p.xaxis.axis_label = 'Features'\n",
    "    p.yaxis.axis_label = 'Importance'\n",
    "    show(p)\n",
    "                    \n",
    "plot_importance(clf.feature_importances_, feature_lst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision-recall curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_precision_recall_curve(clf, X_test, y_test):\n",
    "    y_score = clf.predict_proba(X_test)[:, 1]\n",
    "    precision, recall, _ = precision_recall_curve(y_test, y_score)\n",
    "    plt.plot(recall, precision, lw = 1, color='navy',label='Precision-Recall curve')\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.title(\"Precision-Recall: Random Forest\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_precision_recall_curve(clf, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_precision_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_ROC(clf, X_test, y_test):\n",
    "    y_score = clf.predict_proba(X_test)[:, 1]\n",
    "    fpr_rt, tpr_rt, _ = roc_curve(y_test, y_score)\n",
    "    roc_auc = auc(fpr_rt, tpr_rt)\n",
    "\n",
    "    plt.figure()\n",
    "    lw = 2\n",
    "    plt.plot(fpr_rt, tpr_rt, color='darkorange',lw=lw, label='Random Forest (area = %0.2f)' % roc_auc)\n",
    "\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver operating characteristic example')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "plot_ROC(clf, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### In the reduced feature space (with only the important features taken into account), we can check whether it is ok to use the reduced feature space or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lst_b_in_lst_a(lst_b, lst_a):\n",
    "    mask = []\n",
    "    for i in lst_a:\n",
    "        if i in lst_b:\n",
    "            mask.append(True)\n",
    "        else:\n",
    "            mask.append(False)\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Features, of which the importance scores are larger than 0.001, were selected. The confusion matrix and other metrics were plotted. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(index = feature_lst)\n",
    "df['importance'] = pd.Series(clf.feature_importances_ , index = feature_lst)\n",
    "df_important_ = df[df['importance']>=0.001]\n",
    "mask = [[bool(re.search('tenant', i))] for i in df_important_.index]\n",
    "important_tenant_name_lst = df_important_[mask].index\n",
    "#print important_tenant_name_lst\n",
    "feature_lst_reduced = ['client_note_total_count', 'sub_duration', 'last_sub_duration', 'amount_per_day', 'sentiment']\\\n",
    "    + list(important_tenant_name_lst)\n",
    "#print feature_lst\n",
    "X_original = tb_res[feature_lst_reduced].as_matrix()\n",
    "y = np.array(tb_res['churned'])\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X_original)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.3)\n",
    "\n",
    "clf_reduced = RF(feature_lst_reduced, X_train, y_train, n_estimators = 1000, max_depth = 10)\n",
    "y_pred = clf_reduced.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_importance(clf_reduced.feature_importances_, feature_lst_reduced)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_table_plot(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_score(y_test, y_pred) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recall is one of the important metric to consider, because the algorithm would not want to predict unhappy customers as happy ones. Then, the unhappy customer will be likely to churn. F1 score as a metric indicating a geometric mean of precision and recall can be another important metric to consider.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_precision_recall_curve(clf_reduced, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_ROC(clf_reduced, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The average churn rate over all the clients was ~19%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_total_churn = (tb_res['churned']==1).sum()\n",
    "total_clients = len(tb_res)\n",
    "churn_rate = num_total_churn/float(total_clients)\n",
    "print \"total_clients = \", total_clients\n",
    "print \"churn_rate = \", churn_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 19% churn rate (annual) is really high! My algorithm, based on random forest, can correctly predict the churning clients with accuracy 78%. This improved accuracy can help the tenant companies actively target to the churning customers. If the tenant companies could keep all the customers from churning, the new churn rate will be"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Total_test_cases = len(X_test)\n",
    "TN = (np.array(y_test) & np.array(y_pred)).sum()\n",
    "TN/float(Total_test_cases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicted churn was saved in a column 'shurned_pred' in the table tb_res."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tb_res['churned_pred'] = pd.Series(clf.predict(X_raw), index = tb_res.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I looked into whether there is more room to improve the recall. For every tenant company, predicted and true churn numbers were saved in the table, df_churn .  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lst_cl =[]\n",
    "lst_true = []\n",
    "lst_pred = []\n",
    "lst_corr = []\n",
    "# For all tenants.\n",
    "lst_tenants = tb_res['tenant_id'].unique()\n",
    "\n",
    "for tid in lst_tenants:\n",
    "    df_t = tb_res[tb_res['tenant_id'] == tid]\n",
    "    num_cl = len(df_t)\n",
    "    num_true_churn = df_t['churned'].sum()\n",
    "    num_pred_churn = df_t['churned_pred'].sum()\n",
    "    num_corr_churn = (df_t['churned'] & df_t['churned_pred']).sum()\n",
    "    #print num_cl, \"\\t\", num_true_churn, \"\\t\", num_pred_churn, \"\\t\", num_corr_churn\n",
    "    lst_cl.append(num_cl)\n",
    "    lst_true.append(num_true_churn)\n",
    "    lst_pred.append(num_pred_churn)\n",
    "    lst_corr.append(num_corr_churn)\n",
    "    \n",
    "df_churn = pd.DataFrame(index = lst_tenants)\n",
    "df_churn['total_clients'] = lst_cl\n",
    "df_churn['true_churn'] = lst_true\n",
    "df_churn['pred_churn'] = lst_pred\n",
    "df_churn['correctly_pred_churn'] = lst_corr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature importance scores of individual tenant companies were saved in the table df_churn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_importances = pd.DataFrame(index = feature_lst)\n",
    "df_importances['importance'] = clf.feature_importances_\n",
    "mask = [[bool(re.search('tenant', i))] for i in df_importances.index]\n",
    "df_importances_only_tenants = df_importances[mask].copy(deep=True)\n",
    "\n",
    "new_index = [int(re.match(r'^tenant(.*)', i_str).group(1)) for i_str in df_importances_only_tenants.index]\n",
    "df_importances_only_tenants.index = new_index\n",
    "df_importances_only_tenants\n",
    "df_churn['importance'] = df_importances_only_tenants['importance']\n",
    "\n",
    "p = figure(plot_width=800, plot_height=200, \\\n",
    "           y_range = Range1d(end=0.03), x_range=Range1d(end=60),\\\n",
    "           #y_axis_type=\"log\", x_axis_type =\"log\"\\\n",
    "          )  \n",
    "p.circle(df_churn['true_churn'], df_churn['importance'])\n",
    "p.xaxis.axis_label = 'Number of Customer Churn Cases'\n",
    "p.yaxis.axis_label = 'Importance Scores'\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The graph above clearly showed that the importance scores are closely related to the number of churns, actually quite linearly. Thus, I segmented the tenants into two groups with the number of churns less than or larger than 10. The optimal threshold values can be found with manual tuning. The table df_churn showed that for some tenant companies such as ID = 173 and 132, churn was not predicted at all. But, with the segmentation of the tenant companies, you will see the dramatic enhancement of the prediction recall (refer to \"classification_large_churn_cases\"). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_churn.sort_values('true_churn', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RF for two different groups (many churn tenants vs. small churn tenants)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_small_churns = df_churn[df_churn['true_churn']<10].copy(deep=True)\n",
    "df_large_churns = df_churn[df_churn['true_churn']>=10].copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lst = []\n",
    "for i in df_small_churns.index:\n",
    "    lst.append(tb_res[tb_res['tenant_id'] == i])\n",
    "mask = lst_b_in_lst_a(list(pd.concat(lst).index), list(tb_res.index))\n",
    "tb_res_small = tb_res[mask].copy(deep=True)\n",
    "tb_res_large = tb_res[[not x for x in mask]].copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb_res_small.to_sql(con=engine, name='kim_small_churn_tenants1', if_exists='replace')\n",
    "tb_res_large.to_sql(con=engine, name='kim_large_churn_tenants1', if_exists='replace')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
